{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 算法实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"post and label\"\"\"\n",
    "    posting_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                    ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                    ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                    ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                    ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                    ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    class_vector = [0, 1, 0, 1, 0, 1]  #1 is abusive, 0 not\n",
    "    return posting_list, class_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def create_vocabulary_list(dataset):\n",
    "    vocabulary = set([])\n",
    "    for document in dataset:\n",
    "        vocabulary = vocabulary | set(document)\n",
    "    return list(vocabulary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def word_to_vector(vocab_list, input_set):\n",
    "    return_vector = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vector[vocab_list.index(word)] = 1\n",
    "    return return_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_posts, list_classes = load_dataset()\n",
    "vocabulary = create_vocabulary_list(list_of_posts)\n",
    "word_to_vector(vocabulary, list_of_posts[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_matrix, train_category):\n",
    "    number_of_docs = len(train_matrix)\n",
    "    number_of_words = len(train_matrix[0])\n",
    "    p_abusive = np.sum(train_category) / number_of_docs\n",
    "    p0_num, p1_num = np.ones(number_of_words), np.ones(number_of_words)  # 每个单词出现的次数,避免某个单词出现次数为零\n",
    "    p0_de_num, p1_de_num = 2, 2\n",
    "    for i in range(number_of_docs):\n",
    "        if train_category[i] == 1:\n",
    "            p1_num += train_matrix[i]\n",
    "            p1_de_num += sum(train_matrix[i])\n",
    "        else:\n",
    "            p0_num += train_matrix[i]\n",
    "            p0_de_num += sum(train_matrix[i])\n",
    "    p0_vec = np.log(p0_num / p0_de_num)\n",
    "    p1_vec = np.log(p1_num / p1_de_num)\n",
    "    return p0_vec, p1_vec, p_abusive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "train_matrix = []\n",
    "for docs in list_of_posts:\n",
    "    train_matrix.append(word_to_vector(vocabulary, docs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "p0_vec, p1_vec, p_abusive = train_naive_bayes(train_matrix, list_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_abusive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def classify_naive_bayes(vector, p0_vector, p1_vector, p_abusive):\n",
    "    p0 = np.sum(vector * p0_vector) + np.log(p_abusive)\n",
    "    p1 = np.sum(vector * p1_vector) + np.log(1 - p_abusive)\n",
    "    if p0 > p1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def test():\n",
    "    list_of_posts, list_classes = load_dataset()\n",
    "    vocabulary = create_vocabulary_list(list_of_posts)\n",
    "    train_matrix = []\n",
    "    for post in list_of_posts:\n",
    "        train_matrix.append(word_to_vector(vocabulary, post))\n",
    "    p0_vec, p1_vec, p_abusive = train_naive_bayes(train_matrix, list_classes)\n",
    "    test_input = ['stupid', 'garbage']\n",
    "    test_vector = np.array(word_to_vector(vocabulary, test_input))\n",
    "    return classify_naive_bayes(test_vector, p0_vec, p1_vec, p_abusive)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
