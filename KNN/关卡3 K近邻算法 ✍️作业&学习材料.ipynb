{"cells":[{"cell_type":"markdown","metadata":{"id":"F79B881A1AE94F018CE54D785266D6F1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"# ✍️第三关 - K近邻算法\n\n本训练营所有课程数据，作业练习都在和鲸社区数据分析协作平台 ModelWhale 上，\n点击 [查看学习任务](https://www.heywhale.com/home/activity/detail/62a07f19ac8fed662502782f/content/1)。\n\n点击右上角的蓝色🔵Fork按钮， Fork 拷贝关卡材料后在线运行，即可开始写作业\n\n如☝️第一次参加训练营，不知道如何提交作业可以看👉[💁【步骤图】怎样学习并提交我的作业](https://www.heywhale.com/home/competition/forum/62aaefdba00fb039f752b4f9)？"},{"cell_type":"markdown","metadata":{"id":"8991A2F360764B1C8E8FCD722F30BAAA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"## 一、K近邻算法基础介绍"},{"cell_type":"markdown","metadata":{"id":"640336FF42234E6CA57B7D1F4F9A6D02","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"K近邻算法也是常说的KNN算法，是一种常见的分类和回归算法，当然我们常将其用于分类。是一种监督算法，该算法的内容其实和名字很像，根据邻居来进行判断。有点近朱者赤近墨者黑的意味。\n\n比如我们常说：某个人的工资一般是与其玩的最好的5个朋友(或者说是N个)工资的平均值。\n\n在K近邻算法中也是如此，某个样本的类别是与其**最近**的**K个样本**类别的**众数**，某个样本的值(回归问题中)是与其**最近**的**K个样本**的**平均值**。"},{"cell_type":"markdown","metadata":{"id":"EF51CCDAF3CE48F19C0EA7E4AB0F79EF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"---------------------------------------------------------------------------------------------------------------------------"},{"cell_type":"markdown","metadata":{"id":"9C0F07929A5E4E57B71A52D2694CDC2F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"上句话中我将三个地方标黑了，这也是K近邻算法中最重要的三个地方，也是**KNN算法的三要素**：\n\n(1)：k值的选择\n\n(2)：距离的度量(即何为最近)\n\n(3)：决策规则(为什么分类用众数，回归用平均值)"},{"cell_type":"markdown","metadata":{"id":"0143D862062646CEA02997DE564EADEE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"算法介绍：\n\n以使用KNN算法分类为例，给定一个数据集：\n\n\n$$\nT=\\{(x_1,y_1),(x_2,y_2),...(x_n,y_n)\\}\n$$\n\n\n其中$x_i$是一个k维向量，$y_i\\in \\{c_1,c_2,c_3,...c_k\\}$为数据$x_i$所属的类别。一般$k<<n$\n\n预测：\n\n给定一个$x$输出其所属的类别。\n\n该算法采用的方法是：**在给定的距离度量基础上，在训练集$T$中找出与$x$最接近的k个点，将其记录为$N_k(x)$。然后在$N_k(x)$中根据分类决策规则决定$x$的类别$y$**\n\n用公式表示就是:\n\n\n$$\ny=argmax_{c_j}\\sum_{x_i \\in N_k(x)}I(y_i==c_i)\n$$\n\n\n$I$是指示函数，当$y_i==c_j$时为1，否则为0。\n"},{"cell_type":"markdown","metadata":{"id":"8DCC27A9387743528B6673441B7EC672","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"当$k==1$时，可以称之为最近邻算法，此时$x$的预测值为直接距离$x$最近的训练集中的样本$x_j$对应得$y_j$"},{"cell_type":"markdown","metadata":{"id":"00CBA584BA474024A48391952D557806","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"------------------------------------------------------------------------------------------------------------------------"},{"cell_type":"markdown","metadata":{"id":"5DF5053DC183434DB04DDB9BC461DC9B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"根据上面的介绍，我们可以发现KNN算法好像并不需要对样本$T$进行训练，当预测样本来的时候我们直接查找与预测样本最近，然后做一些判断就ok了。事实也是如此，该算法没有显示学习的过程，并且是是一个无参算法。\n\n无参算法对应的就是有参数算法，故名思义，无参就是没有参数！我们不需要去优化某个参数，然后生成由参数和结构组成的模型。有参算法就是需要训练生成参数，然后根据生成的参数和模型对预测样本进行预测。"},{"cell_type":"markdown","metadata":{"id":"CC567E4B56D24BB1A950FAFC0F2E2A92","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"## 二、K近邻算法三要素和源码"},{"cell_type":"markdown","metadata":{"id":"DFA40E46101E4E3490FE36C50C28512E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"上文说到k近邻算法的三要素。下面我们来详细介绍一下："},{"cell_type":"markdown","metadata":{"id":"771725DDF3C54569821048C01ECA1A00","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"### 1：k值\n\nk值就是选取邻居的数量。到达到达根据几个朋友的工资来估算某个人的工资呢？如果k的取值太小的话，相应的误差就会比较大。反之如果k的取值太大，可能会选取大不那么近的邻居，导致误差也偏大。极端情况下，当k等于样本总和N时，无论预测样本是什么都会选取训练集中最多的类别(分类问题)作为预测结构。"},{"cell_type":"markdown","metadata":{"id":"61A99C2AD8334F64B92B0B6206F6ED9F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"**k值的选取我们可以根据交叉检验的方法来选取**。比如：我们将训练集拆成2份，一部分作为训练集，一部分作为验证集。选取几个k值的候选值，然后预测验证集在该k值情况下的分类误差。然后可以选取分类误差最小的一个k值。当然也可以采用五折验证的方法。"},{"cell_type":"markdown","metadata":{"id":"07D2B09EF70B43A38A62DE3985946620","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f","mdEditEnable":false},"source":"### 2：距离度量"},{"cell_type":"markdown","metadata":{"id":"F92BFCA0A99A4B13B49453E0BF8802F5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"距离度量就是衡量两个样本距离的方法，常见的度量方法有：欧式距离，曼哈顿距离，或者是更为一般的$L_p$距离。\n\n假设特征空间是n维的向量空间$R^n$。$x_i=(x_i^{(1)},x_i^{(2)},...x_i^{(n)})$,$x_j=(x_j^{(1)},x_j^{(2)},...x_j^{(n)})$。$x_i,x_j$的$L_p$距离为：\n\n\n$$\nL_p(x_i,x_j)=(\\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}\n$$\n\n\n其中$q>=1$，可以发现，当$p=2$时，称之为欧式距离：\n\n\n$$\nL_2(x_i,x_j)=(\\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{\\frac{1}{2}}\n$$\n\n\n当$p=1$时，称之为曼哈顿距离，即：\n\n\n$$\nL_2(x_i,x_j)=\\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|\n$$\n"},{"cell_type":"markdown","metadata":{"id":"A6BEF547D27E49FDACAC955010A0C000","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"### 3：决策规则"},{"cell_type":"markdown","metadata":{"id":"29C78B6DD15445B18F72900A71C6B7EA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"就是决策方法，假设一个分类问题，我们现在已经获取了预测样本最近的K个训练样本的类别，怎样去进行预测？**一般情况下我们可以使用直接表决的方法，也就是取众数，在回归问题中我们采用平均数**。当然有时候我们也要考虑到距离因素，给距离越近的样本加点权重。"},{"cell_type":"markdown","metadata":{"id":"47C649AB6F274CC2A567ED55B5F7C5DB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"下面我们用代码来表示从五折检验选取最优的k值，然后通过k值来进行预测。"},{"cell_type":"code","execution_count":1,"metadata":{"id":"04536B3AE86049D899493A95AD58F91C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[{"output_type":"stream","text":"当前K的值为： 2 预测得分为： 89\n当前K的值为： 3 预测得分为： 89\n当前K的值为： 4 预测得分为： 89\n当前K的值为： 5 预测得分为： 89\n当前K的值为： 6 预测得分为： 88\n当前K的值为： 7 预测得分为： 89\n当前K的值为： 8 预测得分为： 89\n当前K的值为： 9 预测得分为： 89\n当前K的值为： 10 预测得分为： 89\n当前K的值为： 11 预测得分为： 89\n当前K的值为： 12 预测得分为： 89\n当前K的值为： 13 预测得分为： 90\n当前K的值为： 14 预测得分为： 88\n当前K的值为： 15 预测得分为： 89\n选择的k为： 13\n预测结果的正确个数为: 56\n预测结果的错误个数为: 4\n","name":"stdout"}],"source":"import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\nX = datasets.load_iris()['data']\nY = datasets.load_iris()['target']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, stratify=Y,random_state=100)\n\n\nclass KNN:\n    def __init__(self, train_x, train_y, test_x, test_y):\n        \"\"\"\n        KNN初始话\n        :param train_x:训练集X\n        :param train_y: 训练集Y\n        :param test_x: 预测集X\n        :param test_y: 预测集Y\n        \"\"\"\n        self.train_x = train_x\n        self.train_y = train_y\n        self.test_x = test_x\n        self.test_y = test_y\n        self.k = None\n\n    def euclidean_dis(self, x1, x2):\n        \"\"\"\n        返回x1与x2的距离(x1,x2均为二维矩阵).x1.shape=(N1*M),x2.shape=(N2*M2),返回结果为(N1*N2)\n        :param x1:\n        :param x2:\n        :return:\n        \"\"\"\n        n1, m1 = x1.shape\n        n2, m2 = x2.shape\n        if m1 != m2:\n            raise (\"两个向量维度不相等\")\n        x1x2 = np.dot(x1, x2.T)  # (n1,n2)\n        y1 = np.repeat(np.reshape(np.sum(np.multiply(x1, x1), axis=1), (n1, 1)), repeats=n2, axis=1)\n        y2 = np.repeat(np.reshape(np.sum(np.multiply(x2, x2), axis=1), (n2, 1)), repeats=n1, axis=1).T\n        dis = y1 + y2 - 2 * x1x2\n        return dis\n\n    def predict(self, train_x, y, test, k):\n        \"\"\"\n        返回根据KNN预测的结果\n        :param train_x: 训练集x\n        :param y: 训练集y\n        :param test: 预测集\n        :return: 返回test预测的结果\n        \"\"\"\n        dis = self.euclidean_dis(test, train_x)\n        k_neighbor = np.argsort(dis, axis=1)[:, :k]\n        k_neighbor_value = y[k_neighbor]\n        n = test.shape[0]  # 预测结果的个数\n        pred = np.zeros(n)\n        for i in range(n):\n            pred[i] = np.argmax(np.bincount(k_neighbor_value[i]))\n        return pred\n\n    def KFlod(self, k):\n        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1996)\n        oof = np.zeros(self.train_x.shape[0])\n        for fold_, (train_index, test_index) in enumerate(folds.split(self.train_x, self.train_y)):\n            train_x, test_x, train_y, test_y = self.train_x[train_index], self.train_x[test_index], \\\n                                               self.train_y[\n                                                   train_index], self.train_y[test_index]\n            pred = self.predict(train_x, train_y, test_x, k)\n            oof[test_index] = pred\n        return np.sum(oof == self.train_y)\n\n    def selectK(self):\n        ks = [2,3, 4, 5, 6,7,8,9,10,11,12,13,14,15]\n        value = 0\n        for k in ks:\n            value_tem = self.KFlod(k)\n            print(\"当前K的值为：\", k, \"预测得分为：\", value_tem)\n            if value_tem > value:\n                self.k = k\n                value = value_tem\n\n    def trainAndPredic(self):\n        self.selectK()\n        print(\"选择的k为：\", self.k)\n        preds = self.predict(self.train_x, self.train_y, self.test_x, self.k)\n        print(\"预测结果的正确个数为:\", np.sum(preds == self.test_y))\n        print(\"预测结果的错误个数为:\", np.sum(preds != self.test_y))\n\n\nmodel = KNN(X_train, y_train, X_test, y_test)\nmodel.trainAndPredic()\n"},{"cell_type":"markdown","metadata":{"id":"54154159601944F7A9953168CF513E2D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"## 三、K近邻算法的优化"},{"cell_type":"markdown","metadata":{"id":"8C44895C2D9C48D494F71C675FE1A7B8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"假设我的样本空间是$N X M$,那么我每次预测一个样本花费的时间是$O(NM)+O(KN)$。(前者为计算距离的时间，后者为选出K个最近邻居的时间)。在很多大型的场景中$N$的值会非常的大，采用该算法进行预测花费的时间会非常的长。比较常见的优化方法有：KD树。在实际的应用中我们会比较常用Annoy等信息检索中常用的方法。"},{"cell_type":"markdown","metadata":{"id":"75AC61571C6F4836A08E21C31279B4FF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"KD树在很多关于K近邻算法的介绍中都有所提及，并且相比与Annoy相比，Annoy算法其实更加常用。\n\nAnnoy也是采用二叉树的方法，在各种工业系统中使用的比较多，算法基础我们不说了，直接看下面的例子："},{"cell_type":"code","execution_count":2,"metadata":{"id":"C0153533180245358EABD0F94CCF59A0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[],"source":"import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nX=datasets.load_iris()['data']\nY=datasets.load_iris()['target']\nY[Y>1]=1\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.4,stratify=Y)"},{"metadata":{"id":"F156381185E1459F9A17D0579C2A0E62","notebookId":"62b137ecd5964bb28b31988f","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"!pip install annoy","execution_count":18},{"cell_type":"code","execution_count":19,"metadata":{"id":"09CFF767EDD2406F9FB70050A39AC9BD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[],"source":"from annoy import AnnoyIndex\n# pip install annoy 安装Annoy"},{"cell_type":"markdown","metadata":{"id":"28D57D20E8CC4A5E921E908A5B3F5169","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"f指的是向量的维度，metric 表示度量公式。在这里，Annoy 支持的度量公式包括：\"angular\", \"euclidean\", \"manhattan\", \"hamming\", \"dot\""},{"cell_type":"code","execution_count":20,"metadata":{"id":"BA07F78969844BCF96849A0386F25B34","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[],"source":"metric=\"euclidean\" \nf=4\na = AnnoyIndex(f, metric)\n"},{"cell_type":"markdown","metadata":{"id":"A980EE0B07DF44AF8BD2603FF9E24D83","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"a.add_item(i, v)：i 是一个非负数，表示 v 是第 i 个向量"},{"cell_type":"code","execution_count":21,"metadata":{"id":"49A7BBCD48074525811C6AC49125B246","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[],"source":"for i in range(90):\n    a.add_item(i, X_train[i])"},{"cell_type":"markdown","metadata":{"id":"F87E54F7F484421C8CBD2E6D022AF0F0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"trees表示树的数量"},{"cell_type":"code","execution_count":22,"metadata":{"id":"FB3B294C33A4476F9B1307CD27C57443","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":22}],"source":"# 树的数量\ntrees=10\na.build(trees) # 10 trees"},{"cell_type":"markdown","metadata":{"id":"6F9DD1DA41DD4F5684E7304BB96D6A57","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"储存索引成文件"},{"cell_type":"code","execution_count":23,"metadata":{"id":"A64902D2BC714164AD87B033BC7AE57C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":23}],"source":"a.save(\"train.ann\")"},{"cell_type":"markdown","metadata":{"id":"594C5ED0C7E14552AF550EA14D48EEC2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"读取存储好的索引文件"},{"cell_type":"code","execution_count":24,"metadata":{"id":"87E3F4037F0747799E0F196716BC623E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":24}],"source":"u = AnnoyIndex(f, 'euclidean')\nu.load(\"train.ann\")"},{"cell_type":"markdown","metadata":{"id":"A2B6D0678EF04429A6BEE4BA03E66849","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"查找距离某个向量最近的n个样本。include_distances=False时仅输出索引，True输出索引和距离值。"},{"cell_type":"code","execution_count":25,"metadata":{"id":"B56FE7DA35AC42F8AE7001DF804FF83F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":"([38, 8, 21, 53, 27, 24, 80, 7, 26, 1],\n [0.3605552017688751,\n  0.3872984051704407,\n  0.469041645526886,\n  0.5196153521537781,\n  0.5477224588394165,\n  0.5744560956954956,\n  0.5830951929092407,\n  0.5916081070899963,\n  0.5999999642372131,\n  0.6480741500854492])"},"metadata":{},"execution_count":25}],"source":"a.get_nns_by_vector(X_test[0], n=10, search_k=-1, include_distances=True)"},{"cell_type":"markdown","metadata":{"id":"048107C16A0D427DB7235EBF363AD963","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"## 四、作业\n\n### STEP1: 按照要求计算下方题目结果"},{"cell_type":"markdown","metadata":{"id":"8CD5D89AE60B434B99743208959861A1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"1：请你回顾下KNN算法的三要素："},{"cell_type":"code","execution_count":26,"metadata":{"id":"95F11D6565174776A0C75C70A1881446","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[],"source":"a1=\"\" # k值\na2=\"\" # 度量距离\na3=\"\" # 决策规则"},{"cell_type":"markdown","metadata":{"id":"4F3F635D05DC42A3A6B9A7E3852A99BB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"2：改写一下函数使其可以应用于KNN的回归预测,回归预测的loss为平方差"},{"cell_type":"code","execution_count":27,"metadata":{"id":"E762C5DE31674C6BA8F0DB593C1D45EA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[],"source":"def predict(self, train_x, y, test, k):\n    \"\"\"\n    返回根据KNN预测的结果\n    :param train_x: 训练集x\n    :param y: 训练集y\n    :param test: 预测集\n    :return: 返回test预测的结果\n    \"\"\"\n    dis = self.euclidean_dis(test, train_x)\n    k_neighbor = np.argsort(dis, axis=1)[:, :k]\n    k_neighbor_value = y[k_neighbor]\n    n = test.shape[0]  # 预测结果的个数\n    pred = np.zeros(n)\n    for i in range(n):\n        pred[i] = np.argmax(np.bincount(k_neighbor_value[i])) ##改写1\n    return pred\n\n\ndef KFlod(self, k):\n    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1996)\n    oof = np.zeros(self.train_x.shape[0])\n    for fold_, (train_index, test_index) in enumerate(folds.split(self.train_x, self.train_y)):\n        train_x, test_x, train_y, test_y = self.train_x[train_index], self.train_x[test_index], \\\n                                           self.train_y[\n                                               train_index], self.train_y[test_index]\n        pred = self.predict(train_x, train_y, test_x, k)\n        oof[test_index] = pred\n    return np.sum(oof == self.train_y)                     #改写2"},{"cell_type":"markdown","metadata":{"id":"078112F3331A4BE1BAC59BD317CEE413","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"下面那个改写是正确的：\n\nA：pred[i]=np.sum(k_neighbor_value[i]),np.sum(oof - self.train_y) \n\nB: pred[i]=np.mean(k_neighbor_value[i]),np.sum(oof - self.train_y) \n\nC：pred[i]=np.sum(k_neighbor_value[i]),np.sum((oof - self.train_y)**2) \n\nD: pred[i]=np.mean(k_neighbor_value[i]),np.sum((oof - self.train_y)**2) "},{"cell_type":"code","execution_count":28,"metadata":{"id":"FEC5DCB06427432395056B757C9A65EB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[],"source":"a4=\"\""},{"cell_type":"markdown","metadata":{"id":"C81DC055E2CA4D7C94D5B381442C31A2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"3:使用全部iris数据，选取中最优的K值，并且计算此时分类正确的个数"},{"cell_type":"code","execution_count":29,"metadata":{"id":"8E26ADD7937F4F3B86B52700369236BF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[],"source":"a5=\"\" # \na6=\"\" # "},{"cell_type":"markdown","metadata":{"id":"D91769F965684025AA4F96296C5E59F7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"source":"### STEP2: 将结果保存为 csv 文件\ncsv 需要有两列，列名：id、answer。其中，id列为题号，从a1开始到a6来表示。answer 列为各题你得出的答案选项。"},{"cell_type":"code","execution_count":30,"metadata":{"id":"63F12AF0582F452F93AD3BF24282E6B2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":true,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":"   id answer\n0  a1       \n1  a2       \n2  a3       \n3  a4       \n4  a5       \n5  a6       ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a1</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a2</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a3</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>a4</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a5</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>a6</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":30}],"source":"import pandas as pd\n\nanswer=[a1,a2,a3,a4,a5,a6]\n\nanswer=[x.upper() for x in answer]\ndic={\"id\":['a'+str(i+1) for i in range(6)],\"answer\":answer}\ndf=pd.DataFrame(dic)\ndf.to_csv('answer3.csv',index=False, encoding='utf-8-sig') #文件名最好别改\ndf"},{"cell_type":"markdown","source":"### STEP3: 提交 csv 文件，获取分数结果\n现在你的答案文件已经准备完毕了，怎么提交得到评分呢？\n\n1、拷贝提交 token\n\n去对应关卡的[提交页面](https://www.heywhale.com/home/activity/detail/62a07f19ac8fed662502782f/submit)，（每个关卡的 token 不一样！）找到对应提交窗口，看到了你的 token 嘛？\n\n拷贝到下方 cell 里（替换掉 XXXXXXX）。\n\n2、找到你的答案文件路径\n\n左侧文件树，在 project 下找到 csv 答案文件，右键点击可复制路径。\n本关的答案路径👇\n\n/home/mw/project/answer3.csv","metadata":{"id":"19922FEEDD4848FE8AA853065461B764","notebookId":"62b137ecd5964bb28b31988f","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8F161F514AEF4613B3AB99AC0632EF76","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137ecd5964bb28b31988f"},"outputs":[],"source":"# 运行这个 cell 前记得一定要保证右上角 kernel为 Python 3 的噢\n# 下载提交工具\n!wget -nv -O heywhale_submit https://cdn.kesci.com/submit_tool/v4/heywhale_submit&&chmod +x heywhale_submit\n\n# 运行提交工具\n# 把下方 XXXXXXX 替换为你的 Token，submit_file 为要提交的文件名路径\n# 文件名路径去左侧文件树下，刷新，找到对应的 csv 文件，右键复制路径\n!./heywhale_submit -token XXXXXXX -file /home/mw/project/answer3.csv"},{"cell_type":"markdown","source":"\n😃\n运行成功、显示提交完成后，即可去[提交页面](https://www.heywhale.com/home/activity/detail/62a07f19ac8fed662502782f/submit)看成绩。满分即可进入下一关。\n","metadata":{"id":"6BCAF918B5AC40A180D42BA8FADB19EF","notebookId":"62b137ecd5964bb28b31988f","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":4}