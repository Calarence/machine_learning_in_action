{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F579891DE02406F9AD460A560E8C304",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 一、逻辑回归的基础介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93F82AE0FCB44218A36ADD65903F3259",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "逻辑回归是一个**分类模型**\n",
    "\n",
    "它可以用来预测某件事发生是否能够发生。分类问题是生活中最常见的问题：\n",
    "\n",
    "生活中：比如预测上证指数明天是否会上涨，明天某个地区是否会下雨，西瓜是否熟了\n",
    "\n",
    "金融领域：某个交易是否涉嫌违规，某个企业是否目前是否违规，在未来一段时间内是否会有违规\n",
    "\n",
    "互联网：用户是否会购买某件商品，是否会点击某个内容\n",
    "\n",
    "对于已知的结果，上面问题的回答只有：${0,1}$ 。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "92746A5351824F2F8E54E6D40574C1C7",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们以以下的一个二分类为例，对于一个给定的数据集，存在一条直线可以将整个数据集分为两个部分：\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/rcmzlxkobr.png?imageView2/0/w/960/h/960)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F0090EAB17844E19402AEB74325D4E5",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "此时，决策边界为$w_1x_1+w_2x_2+b=0$，此时我们很容易将$$ h(x)=w_1x_1+w_2x_2+b>0 $$的样本设置为1，反之设置为0。但是这其实是一个感知机的决策过程。逻辑回归在此基础上还需要在加上一层，**找到分类概率与输入变量之间的关系，通过概率来判断类别**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0633EFB16EE47D5B6E5935D5B3D09E6",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 线性回归模型："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2647CE6B347A44058695896806DAE79D",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "$$ h(x)=w^T x +b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4F9E77CB99594053BB39E915028C31EC",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=[0.1,0.2,0.4,0.2]\n",
    "b=0.5\n",
    "def linearRegression(x):\n",
    "    return sum([x[i]*w[i] for i in range(len(x))])+b\n",
    "linearRegression([2,1,3,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BCBBE60D7FC4588B62CC5420D055148",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "在线性模型的基础上加上一个函数$g$，即$h(x)=g(w^T x+b)$。$g(z)=1/(1+e^{-z})$。这个函数就是sigmoid函数，也叫做logistic函数。\n",
    "它可以将一个线性回归中的结果转化为一个概率值。此时$h(x)$表示的就是某件事发生的概率，我们也可以记为$p(Y=1|x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CC924808F42C470A8574BCAC5A4CAF16",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9088770389851438"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "sigmoid(linearRegression([2,1,3,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6954D19D519443E78B132AEEBB03CA44",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可以看一下sigmoid函数的图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "06119420B025422483AC78D3A54320A9",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/upload/rt/06119420B025422483AC78D3A54320A9/rdtdho4tpk.png\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-10, 10, 0.01)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A2ED2A87BE84EFAA48D5AB498CF5F6D",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "通过以上内容我们知道逻辑回归的表达式，那么我们怎么进行优化呢？x是我们输入的参数，对于我们是已知的，预测西瓜是否熟了的话，我们需要知道它的大小，颜色等信息。将其输入到预测模型中，返回一个西瓜是否熟了的概率。**那么对于怎么得到模型中的参数w和b呢？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1210C3490D964974849906610E584050",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 二、逻辑回归的优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEE56EE321374C9FAA3F8F70025D5D8C",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1：逻辑回归的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2164E604D2441E2BF29FA3031E7D61C",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "逻辑回归采用的是交叉熵的损失函数，对于一般的二分类的逻辑回归来说交叉熵函数为：$J(\\theta )=-[yln(y`)+(1-y)ln(1-y`)]$,其中$y`$是预测值。\n",
    "\n",
    "注：有些地方交叉熵的log的底数是2，有些地方是e。由于$\\frac{log_2(x)}{log_e(x)}=log_2(e)$是一个常数，因此无论是啥对于最后的结果都是不影响的，不过由于计算的简便性用e的会比较多一些。\n",
    "\n",
    "实际上我们求的是训练中所有样本的损失，因此：\n",
    "\n",
    "\n",
    "$$\n",
    "J(\\theta )=-\\frac{1}{m}\\sum[y_i ln(y_i`)+(1-y_i )ln(1-y_i`)]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "注：$\\theta$代表的是所有的参数集合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1CBDE4F59F64DA486579EFA5B5E1733",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCC559FC5301452ABC51CD16214F473B",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Q:为什么不采用最小二乘法进行优化？(平方差)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE451ED4B47A4A28BD6B0139AA21E725",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A:因为采用最小二乘法的话损失函数就是非凸了(凸函数的定义是在整个定义域内只有一个极值，极大或者极小，该极值就是全部的最大或者最小)\n",
    "更多详细地解释可以看这里：https://www.zhihu.com/question/65350200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "284BE5FF54464027A38D921265212468",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "后面可能会有不少难以解释或者需要花费很大篇幅去解释的地方，大多数比较延展性的知识，我可能还是会放个链接，有需要的朋友可以当作课外拓展去了解下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D07510F62B34590A5F65953A940AD00",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EA5D47078AA432E81B6AB575AE7E210",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### 注：损失函数的由来\n",
    "\n",
    "在统计学中，假设我们已经有了一组样本(X,Y)，为了计算出能够产生这组样本的参数。通常我们会采用最大似然估计的方法(一种常用的参数估计的方法)。使用到最大似然估计的话，我们还要一个假设估计，这里我们就是假设 $Y$**是服从于伯努利分布的**。\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "P(Y=1|x)=p(x)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P(Y=0|x)=1-p(x)\n",
    "$$\n",
    "\n",
    "\n",
    "由于$Y$服从于伯努利分布，我们很容易就有似然函数：\n",
    "\n",
    "\n",
    "$$\n",
    "L=\\prod[p(x_i)^{y_i}][1-p(x_i)]^ {（1-y_i）}\n",
    "$$\n",
    "\n",
    "\n",
    "为了求解的方便我们可以两边取对数：\n",
    "\n",
    "\n",
    "$$\n",
    "ln(L)=\\sum[y_iln(p(x_i))+(1-y_i)ln(1-p(x_i))]\n",
    "$$\n",
    "\n",
    "\n",
    "大家其实也发现了$L$和$J$两个式子的关系，其实对$L$求最大就相当于对$J$求最小。这个也是大家以后会听到的**最大似然函数和最小损失函数之间**的关系。如果被问道逻辑回归为什么要采用交叉熵作为损失函数也可以回答：**这是假设逻辑回归中$Y$服从于伯努利分布，然后从最大似然估计中推导来的**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A15E5E39B2174A9A86E0FF4B74AA340B",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2：梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A2FA4659B04406192C0FB7B8020FFAB",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "逻辑回归的优化方法是梯度下降法。\n",
    "从数学的角度来说，函数梯度的方向就是函数增长最快的方向，反之梯度的反方向就是函数减少最快的方向。因此我们想要计算一个函数的最小值，就朝着该函数梯度相反的方向前进。\n",
    "我们先简单地介绍一下该算法。\n",
    "假设我们需要优化的函数：$f(X)=f(x_1,...,x_n)$\n",
    "\n",
    "首先我们初始化自变量，从$X^(0)=(x_1^{(0)},...x_n^{(0)})$开始。设置一个学习率$\\eta$。\n",
    "对于任何$i>=0$:\n",
    "\n",
    "如果是最小化$f$\n",
    "\n",
    "\n",
    "$x_1^{i+1}=x_1^{i}-\\eta \\frac{\\partial{f}}{\\partial{x_1}}(x^{(i)})$\n",
    "\n",
    "\n",
    "$x_n^{i+1}=x_n^{i}-\\eta \\frac{\\partial{f}}{\\partial{x_n}}(x^{(i)})$,\n",
    "\n",
    "反之如果求$f$的最大值，则\n",
    "\n",
    "$x_1^{i+1}=x_1^{i}+\\eta \\frac{\\partial{f}}{\\partial{x_1}}(x^{(i)})$,\n",
    "\n",
    "$x_n^{i+1}=x_n^{i}+\\eta \\frac{\\partial{f}}{\\partial{x_n}}(x^{(i)})$,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7626879C99C446C1992836DB4D71E556",
    "jupyter": {},
    "mdEditEnable": true,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "注：\n",
    "很多人可能是听说过**随机梯度下降，批次梯度下降，梯度下降法**。这三者的区别非常简单，就是看样本数据，随机梯度下降每次计算一个样本的损失，\n",
    "然后更新参数$\\theta$，批次梯度下降是每次根据一批次的样本来更新参数，梯度下降是全部样本。一般来说随机梯度下降速度快(每计算一个样本就可以更新一次参数，参数更新的速度极快)。同样的，随机梯度下降的收敛性会比较差，容易陷入局部最优。反过来每次用来参数更新的样本越多，速度会越慢，但是更能够达到全局最优。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DDDF4C9D5D94602B3F81D7515F462D9",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3：逻辑回归的优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B879E772939F4F19A5B4E45DBEFB21A0",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "以上是逻辑回归优化的目标函数$J(w,b )=-\\frac{1}{m}\\sum[y_i ln(\\sigma(w^T x +b))+(1-y_i )ln(1-\\sigma(w^T x +b))]$\n",
    "\n",
    "我们需要优化参数$w,b$，从而使其在我们已知的样本$X,y$上值最小。也就是我们常说的经验风险最小。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "112A4301335D4315B56A3024D2D486D5",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "既然要优化目标函数，那么首先我们需要对$J(w,b)$求导。\n",
    "\n",
    "我们先令 $g=\\sigma(w^T x +b)$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(g)}{\\partial g}=-\\frac{\\partial}{\\partial g}[yln(g)+(1-y)ln(1-g)]=-\\frac{y}{g}+\\frac{1-y}{1-g}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "再令：$a=w^T x +b$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial a}=\\frac{\\partial ({\\frac{1}{1+e^{-a}}})}{\\partial a}=-(1+e^{-a})^{-2}-e^{-a}=\\frac{1}{1+e^{-a}}\\frac{1+e^{-a}-1}{1+e^{-a}}=\\sigma(a)(1-\\sigma (a))=g(1-g)\n",
    "$$\n",
    "\n",
    "\n",
    "可以发现$g=\\sigma(a)$，但是$g$对$a$求导之后居然是 $g(1-g)$，这也是Sigmoid函数特别有意思的一点，在后续的梯度下降优化中，这个性质可以帮助我们减少很多不必要的计算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "539CF993E622476298A22F50B4692F33",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7E5F32E0A224AFC807B8C66AC8064FD",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "有了上面的基础，我们就可以求我们需要优化的参数$w,b$的梯度了。根据链式求导：\n",
    "\n",
    "\n",
    "$$\n",
    " \\frac{\\partial J}{\\partial w}=\\frac{\\partial J}{\\partial g}\\frac{\\partial g}{\\partial a}\\frac{\\partial a}{\\partial w}=(-\\frac{y}{g}+\\frac{1-y}{1-g})g(1-g)x=(g-y)x\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b}=\\frac{\\partial J}{\\partial g}\\frac{\\partial g}{\\partial a}\\frac{\\partial a}{\\partial b}=(-\\frac{y}{g}+\\frac{1-y}{1-g})g(1-g)=(g-y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4CB935721B64D4E9A3B5E7120B689F0",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "以上就是关于逻辑回归优化的介绍。根据上述的公式，我们简单地写一个根据随机梯度下降法进行优化的函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "C4BB98FDFB1F4593A1893F6CAA96D7D3",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前迭代次数为： 0 训练loss: 0.4291929345464963 验证loss: 0.42693038550585577\n",
      "当前迭代次数为： 5 训练loss: 0.11277882862499525 验证loss: 0.1055056827486348\n",
      "当前迭代次数为： 10 训练loss: 0.06494436815119954 验证loss: 0.05882399271400256\n",
      "当前迭代次数为： 15 训练loss: 0.04581966372516797 验证loss: 0.040633498852263875\n",
      "当前迭代次数为： 20 训练loss: 0.035532576486463324 验证loss: 0.031060875191807488\n",
      "当前迭代次数为： 25 训练loss: 0.029098402617809858 验证loss: 0.025180386326062225\n",
      "当前迭代次数为： 30 训练loss: 0.024686442218866525 验证loss: 0.021207356169834532\n",
      "当前迭代次数为： 35 训练loss: 0.021467975425719914 验证loss: 0.018344588919580344\n",
      "当前迭代次数为： 40 训练loss: 0.019013391664316245 验证loss: 0.016183868735692258\n",
      "当前迭代次数为： 45 训练loss: 0.017077651833693537 验证loss: 0.014494941664880414\n",
      "当前迭代次数为： 50 训练loss: 0.015510694661811146 验证loss: 0.013138213591260125\n",
      "当前迭代次数为： 55 训练loss: 0.014215426919527437 验证loss: 0.012024182383760034\n",
      "当前迭代次数为： 60 训练loss: 0.01312621142455474 验证loss: 0.011092840272228175\n",
      "当前迭代次数为： 65 训练loss: 0.012197059676620554 验证loss: 0.010302458048003647\n",
      "当前迭代次数为： 70 训练loss: 0.011394776690578588 验证loss: 0.009623121587215629\n",
      "当前迭代次数为： 75 训练loss: 0.010694791869480873 验证loss: 0.009032828118843856\n",
      "当前迭代次数为： 80 训练loss: 0.010078522514523285 验证loss: 0.00851503351246566\n",
      "当前迭代次数为： 85 训练loss: 0.009531650098820144 验证loss: 0.008057058396406335\n",
      "当前迭代次数为： 90 训练loss: 0.009042960436174407 验证loss: 0.007649021768855624\n",
      "当前迭代次数为： 95 训练loss: 0.008603543458349608 验证loss: 0.00728310915907939\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=datasets.load_iris()['data']\n",
    "Y=datasets.load_iris()['target']\n",
    "Y[Y>1]=1\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.4,stratify=Y)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def cal_grad(y, t):\n",
    "    grad = np.sum(t - y) / t.shape[0]\n",
    "    return grad\n",
    "\n",
    "def cal_cross_loss(y, t):\n",
    "    loss=np.sum(-y * np.log(t)- (1 - y) * np.log(1 - t))/t.shape[0]\n",
    "    return loss\n",
    "\n",
    "class LR:\n",
    "    def __init__(self, in_num, lr, iters, train_x, train_y, test_x, test_y):\n",
    "        self.w = np.random.rand(in_num)\n",
    "        self.b = np.random.rand(1)\n",
    "        self.lr = lr\n",
    "        self.iters = iters\n",
    "        self.x = train_x\n",
    "        self.y = train_y\n",
    "        self.test_x=test_x\n",
    "        self.test_y=test_y\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.a = np.dot(x, self.w) + self.b\n",
    "        self.g = sigmoid(self.a)\n",
    "        return self.g\n",
    "\n",
    "    def backward(self, x, grad):\n",
    "        w = grad * x\n",
    "        b = grad\n",
    "        self.w = self.w - self.lr * w\n",
    "        self.b = self.b - self.lr * b\n",
    "\n",
    "    def valid_loss(self):\n",
    "        pred = sigmoid(np.dot(self.test_x, self.w) + self.b)\n",
    "        return cal_cross_loss(self.test_y, pred)\n",
    "\n",
    "    def train_loss(self):\n",
    "        pred = sigmoid(np.dot(self.x, self.w) + self.b)\n",
    "        return cal_cross_loss(self.y, pred)\n",
    "\n",
    "    def train(self):\n",
    "        for iter in range(self.iters):\n",
    "            ##这里我采用随机梯度下降的方法\n",
    "\n",
    "            for i in range(self.x.shape[0]):\n",
    "                t = self.forward(self.x[i])\n",
    "                grad = cal_grad(self.y[i], t)\n",
    "                self.backward(self.x[i], grad)\n",
    "\n",
    "            train_loss = self.train_loss()\n",
    "            valid_loss = self.valid_loss()\n",
    "            if iter%5==0:\n",
    "                print(\"当前迭代次数为：\", iter, \"训练loss:\", train_loss, \"验证loss:\", valid_loss)\n",
    "model=LR(4,0.01,100,X_train,y_train,X_test,y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0199F4FC75C24F75AB963A6FAF9244B3",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "以上是手写的逻辑回归，用于自己练习就可以啦，通常我们还是会调sklearn包的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C34F3489441F45E4A71FFF54551C41BB",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.21688590e-02, 6.69378477e-01, 3.18452664e-01],\n",
       "       [3.18783065e-02, 7.46131895e-01, 2.21989799e-01],\n",
       "       [3.43828202e-04, 2.37078790e-01, 7.62577382e-01],\n",
       "       [5.29090689e-04, 4.13567497e-01, 5.85903412e-01],\n",
       "       [3.29337290e-04, 2.81221686e-01, 7.18448977e-01],\n",
       "       [9.00741098e-01, 9.92046734e-02, 5.42283370e-05],\n",
       "       [8.05343921e-01, 1.94609817e-01, 4.62621928e-05],\n",
       "       [9.67282864e-02, 7.74831535e-01, 1.28440178e-01],\n",
       "       [7.93043185e-01, 2.06919042e-01, 3.77732243e-05],\n",
       "       [4.12743806e-04, 4.59051241e-01, 5.40536016e-01],\n",
       "       [2.24448595e-03, 5.20988907e-01, 4.76766607e-01],\n",
       "       [5.15225784e-02, 8.20568416e-01, 1.27909005e-01],\n",
       "       [1.89302090e-03, 3.07587504e-01, 6.90519475e-01],\n",
       "       [8.88574049e-01, 1.11415095e-01, 1.08564499e-05],\n",
       "       [9.19195938e-01, 8.07856941e-02, 1.83682014e-05]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=datasets.load_iris()['data']\n",
    "Y=datasets.load_iris()['target']\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.1,stratify=Y)\n",
    "\n",
    "\n",
    "model=LogisticRegression(penalty='l2',\n",
    "                  class_weight=None,\n",
    "                 random_state=None,  max_iter=100)\n",
    "model.fit(X_train,y_train)\n",
    "model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43A6C8C87531450485BA953525720F20",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**penalty**:惩罚系数，也就是我们常说的正则化，默认为\"l2\",也可以使用\"l1\"，之后我们会介绍逻辑回归的l1,l2正则化。\n",
    "\n",
    "**class_weight**:类别权重，一般我们在分类不均衡的时候使用，比如{0:0.1,1:1}代表在计算loss的时候，0类别的loss乘以0.1。这样在0类别的数据过多时候就相当于给1类别提权了。就我个人而言，比起在类别不均衡时采用采用，我更倾向于这个。\n",
    "\n",
    "**max_iter**：最大迭代次数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E399C72577B465787D6C3DE876082A7",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "以上就是逻辑回归的整体介绍，相信大家看完之后对逻辑回归也有一个比较全面的了解。但是逻辑回归是一个很奇怪的算法，表面上看起来比较简单，但是深挖起来知识特别多，千万不要说自己精通逻辑回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60B002DDEEEA4DF5878A598867582D56",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 三、逻辑回归的拓展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19930E9628834C20A1605D285428945A",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1: 逻辑回归的正则化\n",
    "\n",
    "首先我们介绍一下正则化，对这个方面比较的可以跳过。一般我们模型就是训练就是为了最小化经验风险，**正则化就是在这个基础上加上约束(也可以说是引入先验知识)**，这种约束可以引导优化误差函数的时候倾向于选择向满足约束的梯度下降的方向。\n",
    "\n",
    "注：这里我们可以补充一下经验风险，期望风险和结构化风险。\n",
    "经验风险就是训练集中的平均损失，期望风险就是(X，y)联合分布的期望损失，当样本数量N趋向于无穷大时，经验风险也趋向于期望风向。机器学习做的就是通过经验风险取估计期望风险。\n",
    "结构化风险是防止防止过拟合在经验风险的基础上加上正则项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22FA9488EF6E4AE7A9ABD21A33679FCA",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "在逻辑回归中常见的有L1正则化和L2正则化。\n",
    "\n",
    "加上参数w绝对值的和\n",
    "\n",
    "$$\n",
    "L1：J(\\theta )=-\\frac{1}{m}\\sum[y_i ln(y_i`)+(1-y_i )ln(1-y_i`)]+|w|\n",
    "$$\n",
    "\n",
    "加上参数w平方和\n",
    "\n",
    "\n",
    "$$\n",
    "L2：J(\\theta )=-\\frac{1}{m}\\sum[y_i ln(y_i`)+(1-y_i )ln(1-y_i`)]+||w^2||\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AE6107D35F2A48439A6392820887B79F",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "以$L2$为例，我们的优化的目标函数不再是仅仅经验风险了，我们还要在$||w^2||$最小的基础上达到最小的经验风险。用我们生活的例子就是在最小代价的基础上达成一件事，这样肯定最合理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DE5C28DAC4C4E9290D2D74CA2C70711",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "如果只是到这里感觉就很容易了，但是一般别人都会问你：\n",
    "\n",
    "1：$L1$,$L2$正则化有什么理论基础？\n",
    "\n",
    "2：为什么$L1$正则化容易产生离散值？\n",
    "\n",
    "吐槽：就是加上一个$|w|$和$||w^2||$啊，让$w$的取值小一点，有啥可说的。\n",
    "\n",
    "吐槽归吐槽，问题还是要回答的。\n",
    "\n",
    "**当我们假设参数$w$服从于正态分布的时候，根据贝叶斯模型可以推导出$L2$正则化，当我们假设参数$w$服从于拉普拉斯分布的时候，根据贝叶斯模型可以推导出$L1$正则化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBEF08284DC14075AB98607305B46564",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33E35908B5E64380B593A11465FAB592",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "具体的推导如下：\n",
    "\n",
    "逻辑回归其实是假设参数是确定，我们需要来求解参数。贝叶斯假设逻辑回归的参数是服从某种分布的。假设参数$w$的概率模型为$p(w)$。使用贝叶斯推理的话：\n",
    "\n",
    "\n",
    "$$\n",
    "p(w|D)\\prec p(w)p(D|w)=\\Pi^{M}_{j=1}p(w_j)\\Pi^{N}_{i=1}p(D_i|w_j)\n",
    "$$\n",
    "\n",
    "\n",
    "对上式取$log$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&=>argmax_w[\\Sigma^{M}_{j=1}logp(w_j)+\\Sigma^{N}_{i=1}logp(D_i|w)]\\\\\n",
    "&=>argmax_w[\\Sigma^{N}_{i=1}logp(D_i|w)+\\Sigma^{M}_{j=1}logp(w_j)]\\\\\n",
    "&=>argmin_w (-\\frac{1}{N}\\Sigma^{N}_{i=1}logp(D_i|w)-\\frac{1}{N}\\Sigma^{M}_{j=1}logp(w_j))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "前面的式子大家很熟悉，就是逻辑回归的损失函数。现在假设$p(w)$服从某个分布，相当于引入一个先验的知识。如果$p(w)$服从均值为0拉普拉斯分布：\n",
    "\n",
    "$$\n",
    "p(w)=\\frac{1}{2\\lambda}e^{\\frac{-|w|}{\\lambda}}\n",
    "$$\n",
    "\n",
    "将$p(w)$代入上面的式子可以得到：\n",
    "\n",
    "$$\n",
    "\\Sigma^{M}_{j=1}logp(w_j)=>\\Sigma^{M}_{j=1}(-\\frac{|w_i|}{\\lambda}log(\\frac{1}{2\\lambda}))\n",
    "$$\n",
    "\n",
    "将参数$\\frac{1}{\\lambda}log(\\frac{1}{2\\lambda})$用一个新的参数$\\lambda$代替，然后就可以得到我们正则化L1的正则化项：\n",
    "\n",
    "$$\n",
    "\\lambda\\Sigma^{M}_{j=1}|w_i|\n",
    "$$\n",
    "\n",
    "两部分加起来就是逻辑回归L1正则化下的损失函数了。\n",
    "\n",
    "L2正则化也是同样的道理，只不过是假设$p(w)$服从均值为0的正态分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5B439DA05E85450195B3725685D8587B",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D6CA5C0E08346F288B83BFAD500ED78",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "关于$L1$,$L2$等高线图大家肯定也都接触过了。\n",
    "从模型优化的角度上来说：\n",
    "\n",
    "当$w$大于0时$|w|$的导数为1，根据梯度下降，w在更新完逻辑回归的那个参数之后，需要减去$lr*1$。这样会将w往0出趋近，\n",
    "反之当$w$小于0时，根据梯度下降|w|的梯度也会让w往0的方向趋近。直到$w$为$|w|$的梯度也为0了。\n",
    "\n",
    "$L2$并不会出现以上情况，假设我们把一个特征复制1次。复制之前该特征的权重为w1,复制之后使用L1正则化，倾向于将另外一个特征的权重优化为0，而L2正则化倾向于将两个特征的权重都优化为w1/2,因为我们很明显的知道，当两个权重都为w1/2时。$||w^2||$才会最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3645E8BFC50549AFB05CDDF93E94B1FA",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "以上只是我个人的见解，关于这个问题，我在网上也看到了大神们的各种解释，可以参照着看看:https://zhuanlan.zhihu.com/p/50142573"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE9492838FD5498A904B81AC3607622D",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2: 为什么逻辑回归中经常会将特征离散化。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71CDBF557D9343D3A07E279C15F92875",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "这个是工业界中常见的操作，一般我们不会将连续的值作为特征输入到逻辑回归的模型之中，而是将其离散成0，1变量。这样的好处有：\n",
    "\n",
    "1：稀疏变量的内积乘法速度快，计算结果方便存储，并且容易扩展； \n",
    "\n",
    "2：离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。\n",
    "\n",
    "3：逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； \n",
    "\n",
    "4：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； \n",
    "\n",
    "5：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2583180FA59470D85C0FC5E76EAEE67",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 四、作业\n",
    "\n",
    "### STEP1: 按照要求计算下方题目结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06108870E81B46EF8FFD0A7F30BA9A98",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "作业1：逻辑回归的表达式：\n",
    "\n",
    "A: h(x)=wx+b        \n",
    "\n",
    "B：h(x)=wx     \n",
    "\n",
    "C: h(x)=sigmoid(wx+b)     \n",
    "\n",
    "D: h(x)=sigmoid(wx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "16B637F52D5340979263BF8966859972",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a1=\"C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56F59AF063AB4833BBECFE93D5B70018",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA0345203FCC488FB33CB2CD2DC6665B",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "作业2：下面关于逻辑回归的表述是正确的(多选)：\n",
    "\n",
    "A:逻辑回归的输出结果是概率值，在0-1之间\n",
    "\n",
    "B:使用正则化可以提高模型的泛化性\n",
    "\n",
    "C:逻辑回归可以直接用于多分类\n",
    "\n",
    "D:逻辑回归是无参模型\n",
    "\n",
    "E:逻辑回归的损失函数是交叉熵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "2B1813B24F4349A3918EC141906B9814",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a2=\"ABE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54C04208E924460A91478FE68FE35866",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "482BB982D7104376AF9759C7C9C2B78D",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "作业3：计算$y=sigmoid(w1*x1+w2*x2+1)$当   w=(0.2,     0.3)时，样本X=(1,1),y=1的时w1,w2的梯度和loss：\n",
    "(保存3位小数，四舍五入)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "F4EA9AFDC88443C6B589D4AA0CAA4237",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = np.array([0.2,0.3])\n",
    "x_arr = np.array([1,1])\n",
    "b = 1\n",
    "y = np.array([1])\n",
    "linear_result = np.dot(w,x) +b\n",
    "predict = np.array([sigmoid(linear_result)])\n",
    "grad = cal_grad(y_arr,predict)\n",
    "grad = grad * x_arr\n",
    "a5 = round(cal_cross_loss(y,predict),3)\n",
    "a3=round(grad[0],3)#(w1)\n",
    "a4=round(grad[1],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "28C8A4E599C34BC58C45874075A714C2",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(w,x_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "34FA3C5FBC89495985865F1D52A041D3",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.201\n"
     ]
    }
   ],
   "source": [
    "print(a5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9F561912ECCA4C6FA380E7CC8B9EAEDD",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "作业4：在cal_grad梯度函数的基础上加上L2正则化，下面的函数是否正确?(Y/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "464A5A1EA81240C78B368B95A520DBC7",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cal_grad(y, t,x,w):\n",
    "    \"\"\"\n",
    "    x:输入X\n",
    "    y:样本y\n",
    "    t:预测t\n",
    "    w:参数w\n",
    "    \"\"\"\n",
    "    grad = np.sum(t - y) / t.shape[0]\n",
    "    return grad*x+2*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "79311CF47B7847CE917F8096B3FA9302",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a6=\"Y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6338FB5EAB6491487B5EDEC939BC604",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### STEP2: 将结果保存为 csv 文件\n",
    "csv 需要有两列，列名：id、answer。其中，id列为题号，从a1开始到a6来表示。answer 列为各题你得出的答案选项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "1FAE6110306C4F8299C801296BFB4BC9",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>ABE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>-0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4</td>\n",
       "      <td>-0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a5</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a6</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id answer\n",
       "0  a1      C\n",
       "1  a2    ABE\n",
       "2  a3 -0.182\n",
       "3  a4 -0.182\n",
       "4  a5  0.201\n",
       "5  a6      Y"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # 这里使用下pandas，来创建数据框\n",
    "answer=[a1,a2,a3,a4,a5,a6]\n",
    "\n",
    "# answer=[x.upper() for x in answer]\n",
    "dic={\"id\":['a'+str(i+1) for i in range(6)],\"answer\":answer}\n",
    "df=pd.DataFrame(dic)\n",
    "df.to_csv('answer1.csv',index=False, encoding='utf-8-sig')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "514D4A9B56704EFB89ABDA99D9AB52D3",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### STEP3: 提交 csv 文件，获取分数结果\n",
    "现在你的答案文件已经准备完毕了，怎么提交得到评分呢？\n",
    "\n",
    "1、拷贝提交 token\n",
    "\n",
    "去对应关卡的[提交页面](https://www.heywhale.com/home/activity/detail/62a07f19ac8fed662502782f/submit)，（每个关卡的 token 不一样！）找到对应提交窗口，看到了你的 token 嘛？\n",
    "\n",
    "拷贝到下方 cell 里（替换掉 XXXXXXX）。\n",
    "\n",
    "2、找到你的答案文件路径\n",
    "\n",
    "左侧文件树，在 project 下找到 csv 答案文件，右键点击可复制路径。\n",
    "本关的答案路径👇\n",
    "\n",
    "/home/mw/project/answer1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "B1B5374BADA64924ABA81C167E92A4F4",
    "jupyter": {},
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /opt/conda/lib/libcrypto.so.1.0.0: no version information available (required by wget)\n",
      "wget: /opt/conda/lib/libssl.so.1.0.0: no version information available (required by wget)\n",
      "wget: /opt/conda/lib/libssl.so.1.0.0: no version information available (required by wget)\n",
      "2022-10-15 09:02:51 URL:https://cdn.kesci.com/submit_tool/v4/heywhale_submit [10675472/10675472] -> \"heywhale_submit\" [1]\n",
      "Heywhale Submit Tool 4.0.1\n",
      "\n",
      "> 已验证Token\n",
      "> 提交文件 /home/mw/project/answer1.csv (0.06 KiB), Target Qiniu\n",
      "> 已上传 100 %\n",
      "> 文件已上传        \n",
      "> 服务器响应: 200 提交成功，请等待评审完成\n",
      "> 提交完成\n"
     ]
    }
   ],
   "source": [
    "# 运行这个 cell 前记得一定要保证右上角 kernel为 Python 3 的噢\n",
    "# 下载提交工具\n",
    "!wget -nv -O heywhale_submit https://cdn.kesci.com/submit_tool/v4/heywhale_submit&&chmod +x heywhale_submit\n",
    "\n",
    "# 运行提交工具\n",
    "# 把下方 XXXXXXX 替换为你的 Token，submit_file 为要提交的文件名路径\n",
    "# 文件名路径去左侧文件树下，刷新，找到对应的 csv 文件，右键复制路径\n",
    "!./heywhale_submit -token ab7e7554b08e5b1f -file /home/mw/project/answer1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13A8DA6F764447468689B9C008BB503A",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "633273a792d6ccb6b3e907f7",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "😃\n",
    "运行成功、显示提交完成后，即可去[提交页面](https://www.heywhale.com/home/activity/detail/62a07f19ac8fed662502782f/submit)看成绩。满分即可进入下一关。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
