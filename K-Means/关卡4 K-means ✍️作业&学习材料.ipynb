{"cells":[{"cell_type":"markdown","metadata":{"id":"75C231DCE95F438A87FF05E736274C5B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"# ✍️第四关 - K-means算法\n\n本训练营所有课程数据，作业练习都在和鲸社区数据分析协作平台 ModelWhale 上，\n点击 [查看学习任务](https://www.heywhale.com/home/activity/detail/62a07f19ac8fed662502782f/content/1)。\n\n点击右上角的蓝色🔵Fork按钮， Fork 拷贝关卡材料后在线运行，即可开始写作业\n\n如☝️第一次参加训练营，不知道如何提交作业可以看👉[💁【步骤图】怎样学习并提交我的作业](https://www.heywhale.com/home/competition/forum/62aaefdba00fb039f752b4f9)？"},{"cell_type":"markdown","metadata":{"id":"CC1C4980766C49E3A0AA0A2A230AF013","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"## 一、k-means算法的基本介绍"},{"cell_type":"markdown","metadata":{"id":"898FA5B506594A2E81FB86137D7BF32D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"k-means算法是一个十分经典的聚类算法，同时它也是一个无监督的算法。对于一组没有标记的数据，如果我们需要将其分为几组的话，这时候我们就可以使用到K-means算法了。"},{"cell_type":"markdown","metadata":{"id":"206820394CB34753A134294B4C535534","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"给定n个样本的集合X={x_1,x_2,...x_n},每个样本都由一个特征向量表示，特征向量的维度为m。k-means算法的目标就是将这n个样本划分为k个集合(k<<n)，**集合两两之间无交集，k个集合的总和就是样本整体X**。\n\n每个集合都有一个中心点，某个样本距离哪个集合的中心点距离最近就将其划分为某个集合。\n\n两个向量间距离的衡量方法有很多，k-means中一般使用的是欧氏距离：\n\n\n$$\nd(x_i,x_j)=||x_i-x_j||^2\n$$\n\n\nk-means算法的目标是通过损失函数最小化来选取最优的划分策略，它的损失函数就是所有样本到达所属集合的中心点的距离最小：\n\n\n$$\nW(C)=\\sum_{l=1}^{k}\\sum_{C(i)=l}||x_i-x_l^{*}||^2\n$$\n\n\n其中$C(i)=l$表示样本i属于集合l。$X^{*}=(x^{*}_{1},x^{*}_{2},x^{*}_{3}...x^{*}_{k})$表示k个集合的中心点。计算集合中心点的方法我们采用的是：\n\n\n$$\nx^{*}_l=\\frac{1}{\\sum_{C(i)=l}} \\sum_{C(i)=l}x_i\n$$\n\n\n即该集合中所有样本的在所有维度的平均值。"},{"cell_type":"markdown","metadata":{"id":"A090167626F84A45ACC014512B26400F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"k-means算法在理解起来是很简单的，不幸的是，求解起来确是很难的，无法求出最优的解，需要使用到迭代的方法，也就是我们下文中介绍的EM算法"},{"cell_type":"markdown","metadata":{"id":"8A5B0EE2FEA046A98CD08E6BF58A45FE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"## 二、k-means算法的理论基础——EM算法"},{"cell_type":"markdown","metadata":{"id":"A3843CC6D1614D5780690BBFB6A8EC87","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"EM算法时机器学习中乃至统计学中非常常用的一种算法。在各种模型的优化中都会使用到该算法，比如贝叶斯优化，马尔可夫优化等，K-means算法中的优化算法也是EM算法，因此我们首先来介绍一下EM算法。"},{"cell_type":"markdown","metadata":{"id":"08CC8B799F664EA9B9EF6305EE780C9D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"EM算法是一种迭代的算法，用于含有隐变量的概率模型参数的极大似然估计。\n\n什么叫含有隐变量的极大似然估计呢？首先我们来个不含有隐变量的极大似然估计。"},{"cell_type":"markdown","metadata":{"id":"886FB9A09439496F8FACF6507EB8F47E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"#### 不含隐变量的极大似然估计"},{"cell_type":"markdown","metadata":{"id":"EADBBD0938014B8591EB3A63E5BB910B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"假设我投掷一枚硬币10次，出现的正反面的情况为(0,1,1,1,0,1,1,1,1,0)。如果现在想要求该硬币的参数(也就是硬币为正面的概率)\n\n通过极大似然估计我们有：\n\n$$\n\\prod p^7(1-p)^3\n$$\n\n很显然我们可以很简单地知道$p=0.7$时结果最优。\n\n将其推广到K-means算法中，如果我们知道了每个类别的中心点，我们也很容易知道每个样本的类别。"},{"cell_type":"markdown","metadata":{"id":"A7503ACE202B42B4B760D3812A2FF045","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"#### 包含隐变量的极大似然估计"},{"cell_type":"markdown","metadata":{"id":"4D2E3B0A582B42C6826C3AC5F68D3535","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"介绍了无隐变量的极大似然估计，现在我们介绍一个包含隐变量的似然估计(**该例子来自于统计学习方法[李航]中p.175至p.176中的例子**，当然我会引入自己的理解)"},{"cell_type":"markdown","metadata":{"id":"CC65E307C8E8441084E8B1F922DD9714","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"(三硬币模型)假设有A,B,C三枚硬币。它们出现为正面时的概率是$\\pi,p,q$。现在我们需要做一个实验：先投掷A硬币，假设其出现正面就投掷B,反之就投掷C。根据B,C出现的结果，做如下记录：正面记录1，反面记录0。(注意，虽然我们每次实验开始都是投掷A,根据A的结果去选择B或者C，但是A的投掷结果并没有被记录下来。)\n我们独立地进行了10次实验，观测的结果是：\n\n$$\n1，1，0，1，0，0，1，0，1，1\n$$\n"},{"cell_type":"markdown","metadata":{"id":"62BE75D741AC4AD4B8A875DC1B194B11","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"我们只能观测到以上10次结果，**每一次的投掷结果都不知道它由硬币B还是硬币C产生的**，现在我们需要求的是三枚硬币出现正面的概率，也即是三枚硬币的模型参数。"},{"cell_type":"markdown","metadata":{"id":"BF683A2136D64C30B413CB57C155FCB6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"在这个模型中，隐变量就是硬币A的投掷结果，它用户指导投掷硬币B还是硬币C。因此我们有：\n\n\n$$\nP(y|\\theta)=\\sum P(y,z|\\theta)=\\sum P(z|\\theta)P(y|z,\\theta)=\\pi{p^y}(1-p)^{1-y}+(1-\\pi)q^y(1-q)^{1-y}\n$$\n\n\ny代表最后观测并记录下来的结果，即0或者1；z是隐变量，即我们未记录下来的硬币A的观测结果(是我们不知道结果)；$\\theta=(\\pi,p,q)$是整个模型的参数，即A,B,C三枚硬币为正面的概率。\n\n\n$$\nP(1|\\theta)=\\pi{p}+(1-\\pi){q}\n$$\n\n\n$$\nP(0|\\theta)=\\pi{(1-p)}+(1-\\pi){(1-q)}\n$$\n"},{"cell_type":"markdown","metadata":{"id":"B95FE206300A40CA83B2C03865F01179","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"根据极大似然估计我们可以得到：\n\n\n$$\nP(Y|\\theta)=\\prod \\pi{p^y_i}(1-p)^{1-y_i}+(1-\\pi)q^y_i(1-q)^{1-y_i}\n$$\n\n\n\n$Y=(y_1,y_2,...y_n)$为每次观测到的结果。我们需要求解的是在参数$\\theta$为什么的情况下，上式的取值最大。\n\n"},{"cell_type":"markdown","metadata":{"id":"14AEB7AF461E4AD7A0658F5AF47E01B4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"遗憾的是，上式并没有解析解，我们需要通过迭代的方法才能得到一个比较好的近似解。也就是我们说的EM算法。\n\n#### EM算法步骤如下：\n\n(1):初始化模型参数$\\theta$,比如我们可以$\\theta=(\\pi=0.5,p=0.5,q=0.5)$;\n\n(2):E步，记$\\theta^{(i)}$为第i次迭代参数$\\theta$的估计值，在第$i+1$次迭代的E步中，计算：\n\n\n$$\nQ(\\theta,\\theta^{(i)})=E_Z[logP(Y,Z|\\theta)Y,\\theta^{(i)}]=\\sum logP(Y,Z|\\theta)P(Z|Y,\\theta^{(i)})\n$$\n\n\n这个式子看起来有点复杂，其实就是根据我们上一步估计的参数$\\theta^{(i)}$以及观测结果 Y 来估计隐变量的结果。\n\n在我们例子中就是我们现在知道了每个硬币的参数以及最后的观测结果，来估计隐变量$Z$,\n\n\n$$\nP(z_j|\\theta^i,y_j)=\\frac{{\\pi}^i(p^i)^{y_j} (1-p^i)^{(1-y_j)}}{{\\pi}^i(p^i)^{y_j} (1-p^i)^{(1-y_j)}+(1-{\\pi}^i)(q^i)^{y_j}(1-q^i)^(1-y_j)}\n$$\n\n\n$P(z_j|\\theta^i,y_j)$就是确定参数和观测结果y的情况下，观测结果来自于硬币B的概率。我们可以写一个函数func_E来计算。\n\n"},{"cell_type":"markdown","metadata":{"id":"41330B2694D24EE0A19D01D07F11F882","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"(3):M步,求使得$Q(\\theta,\\theta^i )$最大化的$\\theta$，确定第$i+1$次迭代时候的参数值$\\theta^{i+1}$\n\n在E步中我们求出了隐变量的结果，根据隐变量的结果我们同样可以求出参数$\\theta$的值：\n\n\n$$\n\\pi^{i+1}=\\frac{1}{n}\\sum_{j=1}^{n} \\mu_j^{i+1}\n$$\n\n\n\n$$\np^{i+1}=\\frac{\\sum_{j=1}^{n} \\mu_j^{i+1}y_j}{\\sum_{j=1}^{n} \\mu_j^{i+1}}\n$$\n\n\n\n$$\nq^{i+1}=\\frac{\\sum_{j=1}^{n} (1-\\mu_j^{i+1})y_j}{\\sum_{j=1}^{n} (1-\\mu_j^{i+1})}\n$$\n\n\n这三个式子很好理解，我们指导了每次观测结果来自于硬币B的概率之后，很自然地通过极大似然估计可以得到$\\pi,p,q$。\n\n同样地，我们可以写一个函数func_M来计算M步的值\n\n(4)重复EM步，直到收敛"},{"cell_type":"code","execution_count":1,"metadata":{"id":"4753469ED3934EF8B74552370FC27593","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[{"output_type":"stream","name":"stdout","text":"[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n"}],"source":"def func_E(theta,y):\n    pi,p,q=theta\n    if y==1:\n        u=(pi*p)/(pi*p+(1-pi)*q) \n        #当观测结果是1的时候，来自于硬币B的概率就是使用硬币B投掷出1的概率(即pi*p)除以观测结果出现1的概率(pi*p+(1-pi)*q)\n    else:\n        u=(pi*(1-p))/(pi*(1-p)+(1-pi)*(1-q))\n        #当观测结果是0的时候，来自于硬币B的概率就是使用硬币B投掷出0的概率(即pi*(1-p))除以观测结果出现0的概率(pi*(1-p)+(1-pi)*(1-q))\n    return u\nU=[]\nY=[1,1,0,1,0,0,1,0,1,1]\ntheta=(0.5,0.5,0.5)\nfor i in Y:   \n    u=func_E(theta,i)\n    U.append(u)\nprint(U)"},{"cell_type":"code","execution_count":2,"metadata":{"id":"D1CDDD0336654849A2B7AB3F963A2E16","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[{"output_type":"stream","name":"stdout","text":"(0.5, 0.6, 0.6)\n"}],"source":"def func_M(U,Y):\n    pi=sum(U)/len(U)\n    p=sum([U[i]*Y[i] for i in range(len(U))])/sum(U)\n    q=sum([(1-U[i])*Y[i] for i in range(len(U))])/sum([1-i for i in U])\n    return (pi,p,q)\nU=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\nY=[1,1,0,1,0,0,1,0,1,1]\ntheta=func_M(U,Y)\nprint(theta)"},{"cell_type":"markdown","metadata":{"id":"F4E7BD84FC574BAC89C4B831ABB0198B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"将两个函数综合起来，对于该问题，整个模型的代码为："},{"cell_type":"code","execution_count":3,"metadata":{"id":"CF8BC51376C64F2B810692BC832452A1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[{"output_type":"stream","name":"stdout","text":"第1次迭代的结果： (0.40641711229946526, 0.5368421052631579, 0.6432432432432431)\n第2次迭代的结果： (0.40641711229946526, 0.5368421052631579, 0.6432432432432431)\n第3次迭代的结果： (0.40641711229946526, 0.5368421052631579, 0.6432432432432431)\n第4次迭代的结果： (0.40641711229946526, 0.5368421052631579, 0.6432432432432431)\n第5次迭代的结果： (0.40641711229946526, 0.5368421052631579, 0.6432432432432431)\n第6次迭代的结果： (0.40641711229946526, 0.5368421052631579, 0.6432432432432431)\n第7次迭代的结果： (0.40641711229946526, 0.5368421052631579, 0.6432432432432431)\n第8次迭代的结果： (0.40641711229946526, 0.5368421052631579, 0.6432432432432431)\n第9次迭代的结果： (0.40641711229946526, 0.5368421052631579, 0.6432432432432431)\n第10次迭代的结果： (0.40641711229946526, 0.5368421052631579, 0.6432432432432431)\n第1次迭代的结果： (0.45320197044334976, 0.7565217391304346, 0.4702702702702703)\n第2次迭代的结果： (0.45320197044334976, 0.7565217391304346, 0.4702702702702703)\n第3次迭代的结果： (0.45320197044334976, 0.7565217391304346, 0.4702702702702703)\n第4次迭代的结果： (0.45320197044334976, 0.7565217391304346, 0.4702702702702703)\n第5次迭代的结果： (0.45320197044334976, 0.7565217391304346, 0.4702702702702703)\n第6次迭代的结果： (0.45320197044334976, 0.7565217391304346, 0.4702702702702703)\n第7次迭代的结果： (0.45320197044334976, 0.7565217391304346, 0.4702702702702703)\n第8次迭代的结果： (0.45320197044334976, 0.7565217391304346, 0.4702702702702703)\n第9次迭代的结果： (0.45320197044334976, 0.7565217391304346, 0.4702702702702703)\n第10次迭代的结果： (0.45320197044334976, 0.7565217391304346, 0.4702702702702703)\n"}],"source":"class EM:\n    def __init__(self,theta,Y):\n        self.theta=theta\n        self.Y=Y\n        self.u=None\n    def func_E(self,theta):\n        pi,p,q=theta\n        U=[]\n        for i in self.Y:\n            if i==1:\n                u=(pi*p)/(pi*p+(1-pi)*q) \n            else:\n                u=(pi*(1-p))/(pi*(1-p)+(1-pi)*(1-q))\n            U.append(u)\n        return U\n    \n    def func_M(self,U):\n        pi=sum(U)/len(U)\n        p=sum([U[i]*self.Y[i] for i in range(len(U))])/sum(U)\n        q=sum([(1-U[i])*self.Y[i] for i in range(len(U))])/sum([1-i for i in U])  \n        \n        return (pi,p,q)\n    \n    def train(self):\n        for i in range(10):\n            U=self.func_E(self.theta)\n            theta=self.func_M(U)\n            print(\"第{}次迭代的结果：\".format(i+1),theta)\n        \ntheta=(0.4,0.6,0.7)\nY=[1,1,0,1,0,0,1,0,1,1]\nmodel1=EM(theta,Y)\nmodel1.train()\ntheta=(0.4,0.6,0.3)\nmodel2=EM(theta,Y)\nmodel2.train()"},{"cell_type":"markdown","metadata":{"id":"9382B34CCC184987B4A1C6DE925F931C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"上面就是EM算法解决掷硬币问题的所有代码，可以发现初始值的选择不同，对于最后的结果有很大的差异，这也是EM算法的性质所决定的，只能求一个比较好的解。"},{"cell_type":"markdown","metadata":{"id":"5148FC84F2F34DD29E5BC23A4E8671C1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"## 三、使用EM算法来求解K-means"},{"cell_type":"markdown","metadata":{"id":"76563B0DBA5F429491FF973C902ED9FD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"介绍完了EM算法之后，我们就需要使用EM算法来求解k-means算法。在此之前我们先采用EM算法来解释K-means算法。\n\n在k-means算法中我们可以将每个集合中心点视为为隐变量，每个样本的类别是我们最后所求的，可以视为参数。\n\n初始化：在初始化的时候，我们可以先随机初始化所有样本所属的类别。\n\nE步：根据每个样本所属的类别计算每个类别中心点的位置。\n\nM步：根据上一步中每个类别中心点的位置，重新计算每个样本所属的类别\n\n注：在实际的计算中，我们将两步颠倒也是可以的，首先初始化中心点。"},{"cell_type":"code","execution_count":4,"metadata":{"id":"8D74D8ED13034DA7B0DCBB8F9D2436BA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>"},"metadata":{}}],"source":"import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nX=datasets.load_iris()['data'][:,:2]\nimport matplotlib.pyplot as plt\nfig=plt.figure()\nax=fig.add_subplot(111)\nax.scatter(X[:,0],X[:,1])\nplt.show()"},{"cell_type":"markdown","metadata":{"id":"34BD5D44B5344D909AA8666A20076817","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"以上就是我们的数据，我们需要使用EM算法将其分为3类。代码如下所示："},{"cell_type":"code","execution_count":5,"metadata":{"id":"66E0AD81840D4EFC9F1D7F0F7A828694","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[{"output_type":"stream","name":"stdout","text":"3 2\n第一个中心点的位置是： [6.17840909 3.26022727]\n第一个中心点的位置是： [6.50673816 3.14591869]\n第一个中心点的位置是： [6.68459893 3.07492963]\n第一个中心点的位置是： [6.76316536 3.06490249]\n第一个中心点的位置是： [6.80128909 3.06867148]\n第一个中心点的位置是： [6.81252686 3.07434732]\n第一个中心点的位置是： [6.81276098 3.07446557]\n第一个中心点的位置是： [6.81276585 3.07446803]\n第一个中心点的位置是： [6.81276596 3.07446808]\n第一个中心点的位置是： [6.81276596 3.07446809]\n第一个中心点的位置是： [6.81276596 3.07446809]\n第一个中心点的位置是： [6.81276596 3.07446809]\n第一个中心点的位置是： [6.81276596 3.07446809]\n第一个中心点的位置是： [6.81276596 3.07446809]\n第一个中心点的位置是： [6.81276596 3.07446809]\n"}],"source":"class KMeans:\n    def __init__(self,K,X):\n        \"\"\"\n        K:类别的数量\n        X:数据\n        \"\"\"\n        self.K=K\n        self.X=X\n        self.w=X.shape[1] ##数据X的维度\n        print(self.K,X.shape[1])\n        self.center=np.zeros([self.K,X.shape[1]]) ##中心点\n        self.cate=None                      ##每条数据的类别\n        \n    def cal_eclud(self,vec1,vec2):\n        return sum((vec1-vec2)**2)**0.5\n        \n        \n    def initialization(self):\n        #中心点的初始化\n        self.center=self.X[:self.K] #随机将X中前K个值复制给中心点。\n        \n    def update_cate(self):\n        #根据上一步求得的中心点，更新每个样本的类别\n        dis=np.zeros([self.X.shape[0],self.K])\n        for i in range(self.X.shape[0]):\n            for j in range(self.K):\n                dis[i,j]=self.cal_eclud(self.X[i],self.center[j])\n        self.cate=np.argmin(dis,axis=1)\n    \n    def update_center(self):\n        #根据每个样本的类别来重新更新中心点\n        for i in range(self.K):\n            self.center[i]=np.mean(self.X[self.cate==i],axis=0)\n    def train(self):\n        self.initialization()\n        for i in range(15):\n            self.update_cate()\n            self.update_center()\n            print(\"第一个中心点的位置是：\",self.center[0])\n        \nmodel=KMeans(3,X)     \nmodel.train()  \ncate=model.cate"},{"cell_type":"code","execution_count":6,"metadata":{"id":"EAE60A88ED254276A90A0D6D8D173927","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/EAE60A88ED254276A90A0D6D8D173927/rdtgmwl3td.png\">"},"metadata":{"needs_background":"light"}}],"source":"import matplotlib.pyplot as plt\nfig=plt.figure()\nax=fig.add_subplot(111)\nid_1=np.where(cate==0)\np1=ax.scatter(X[id_1,0],X[id_1,1],marker=\".\",color=\"green\",s=180)\nid_2=np.where(cate==1)\np2=ax.scatter(X[id_2,0],X[id_2,1],marker=\".\",color=\"black\",s=180)\nid_3=np.where(cate==2)\np3=ax.scatter(X[id_3,0],X[id_3,1],marker=\".\",color=\"red\",s=180)\nplt.show()"},{"cell_type":"markdown","metadata":{"id":"E2B9A56C44154403A8FFD4054D9C1F22","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"上图就是使用KMeans算法聚类后的结果，从图中还是可以看出，聚类的结果还是比较符合预期的。"},{"cell_type":"markdown","metadata":{"id":"95DD36D458DD4AEBAC372C35E54F6366","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"### 在sklean中使用KMeans算法"},{"cell_type":"code","execution_count":7,"metadata":{"id":"C29808A4E68A432AA3D370392909041A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([2, 2], dtype=int32)"},"metadata":{},"execution_count":7}],"source":"from sklearn.cluster import KMeans\nkmeans=KMeans(n_clusters=3,random_state=1996,n_jobs=4).fit(X)\n\nkmeans.predict([[1,2],[4,5]])"},{"cell_type":"markdown","metadata":{"id":"B28782FE1B1A4207BF16147F3467EE3F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"## 四、Kmeans算法的优化"},{"cell_type":"markdown","metadata":{"id":"A6BAFA367FA3442B932003BD48804732","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"1：初始化的生成\n\n由于EM算法中初始化的不同最后导致的结果会有很大的差异，因此合理的初始化是很有必要的。比起随机初始化，我们可以划分整个样本的向量空间，在整个样本空间内均匀地选择中心点。"},{"cell_type":"markdown","metadata":{"id":"8E8E7A9F498F4FC586A9F14D91357849","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"2：采用kmeans++算法\n\n其实这个算法也只是对初始点的选择有改进而已，其他步骤都一样。初始质心选取的基本思路就是，初始的聚类中心之间的相互距离要尽可能的远。\n\n该算法的步骤是：\n\n步骤一：随机选取一个样本作为第一个聚类中心 c1；\n\n步骤二：计算每个样本与当前已有类聚中心最短距离（即与最近一个聚类中心的距离），用 D(x)表示；这个值越大，表示被选取作为聚类中心的概率较大；最后，用轮盘法选出下一个聚类中心；\n\n步骤三：重复步骤二，知道选出 k 个聚类中心。\n\n简单来说就是从一个中心点出发，每次每次新增一个中心点，指导中心点的距离为k个。"},{"cell_type":"markdown","metadata":{"id":"1D09DDE9C76947BAB43EB2526FA3EBF0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"3：归一化\n\n由于k-means算法常用的距离函数是欧式距离，因此一般需要对样本进行同一维度的归一化，防止某个维度因为量级原因影响过大。"},{"cell_type":"markdown","metadata":{"id":"66C2823B134E4777B92295DA73ABB735","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"## 五、作业\n\n### STEP1: 按照要求计算下方题目结果"},{"cell_type":"markdown","metadata":{"id":"73F20DB00EF640CCBBFA7BAF84470191","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"数据准备："},{"cell_type":"code","execution_count":41,"metadata":{"id":"1C3767F8F9094B669F821FF5EA903824","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[],"source":"import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nX=datasets.load_iris()['data']\nY=datasets.load_iris()['target']"},{"cell_type":"markdown","metadata":{"id":"C88ADEF67C1E43458E719A14845D2F77","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"1：对样本X进行归一化(均值方差归一化)，输出并且将归一化的结果的第一行(**将输出的list转化为string,用逗号(,)连接，四舍五入保留3位小数，注意不能有空格**)"},{"cell_type":"code","execution_count":35,"metadata":{"id":"B6B7B4164B57407787494F67989DD8A6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[],"source":"a1 = \"\""},{"cell_type":"markdown","metadata":{"id":"DD63E2135026422FABFD9D967F7971B1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"2:计算上一步**归一化之后**样本中第一个样本与最后一个样本的欧式距离（四舍五入3位小数）。"},{"cell_type":"code","source":"a2=\"\"","metadata":{"id":"AF6616F78AB14F8389CD95F754168821","notebookId":"62b137eed5964bb28b3198c9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"execution_count":36},{"cell_type":"markdown","metadata":{"id":"38F60A4B50154DD5A8E434B4FCED44E2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"3：根据数据中已有的类别(Y就是类别)，分别计算类别0，类别1，类别2的中心点(不归一化)：(**将输出的list转化为string,用逗号(,)连接，四舍五入保留1位小数，不能有空格**)"},{"cell_type":"code","execution_count":37,"metadata":{"id":"F201EDE6E0574CCE835A552A5E55BA2E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[],"source":"a3=\"\"#(类别Y=0)\na4=\"\"#(类别Y=1)\na5=\"\"#(类别Y=2)"},{"metadata":{"id":"56D7CFDF483A4A018D5BA2EF97799653","notebookId":"62b316ad82643be7102b90e4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"50"},"execution_count":42}],"source":"len(X[Y==0])","execution_count":42},{"cell_type":"markdown","metadata":{"id":"C170F7E4B45341ADAFD52FA7BEF15912","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"source":"### STEP2: 将结果保存为 csv 文件\ncsv 需要有两列，列名：id、answer。其中，id列为题号，从a1开始到a5来表示。answer 列为各题你得出的答案选项。\n\n"},{"metadata":{"id":"8496129BB5264EFABC73B99B00269C15","notebookId":"62b137eed5964bb28b3198c9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"cell_type":"code","execution_count":15,"metadata":{"id":"B1B98D5D7221450EA1D86E6CFCDBF8DC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"notebookId":"62b137eed5964bb28b3198c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":"   id answer\n0  a1       \n1  a2       \n2  a3       \n3  a4       \n4  a5       ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a1</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a2</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a3</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>a4</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a5</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":15}],"source":"import pandas as pd\nanswer=[a1,a2,a3,a4,a5]\nanswer=[x.upper() for x in answer]\ndic={\"id\":['a'+str(i+1) for i in range(5)],\"answer\":answer}\ndf=pd.DataFrame(dic)\ndf.to_csv('answer4.csv',index=False, encoding='utf-8-sig') \ndf\n"},{"cell_type":"markdown","source":"### STEP3: 提交 csv 文件，获取分数结果\n现在你的答案文件已经准备完毕了，怎么提交得到评分呢？\n\n1、拷贝提交 token\n\n去对应关卡的[提交页面](https://www.heywhale.com/home/activity/detail/62a07f19ac8fed662502782f/submit)，（每个关卡的 token 不一样！）找到对应提交窗口，看到了你的 token 嘛？\n\n拷贝到下方 cell 里（替换掉 XXXXXXX）。\n\n2、找到你的答案文件路径\n\n左侧文件树，在 project 下找到 csv 答案文件，右键点击可复制路径。\n本关的答案路径👇\n\n/home/mw/project/answer4.csv\n","metadata":{"id":"D1519D7A5F8E42FC83AA51B1B345DC75","notebookId":"62b137eed5964bb28b3198c9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"metadata":{"id":"738B2F6A4DAE47E19E50E015AB32176D","notebookId":"62b137eed5964bb28b3198c9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 运行这个 cell 前记得一定要保证右上角 kernel为 Python 3 的噢\n# 下载提交工具\n!wget -nv -O heywhale_submit https://cdn.kesci.com/submit_tool/v4/heywhale_submit&&chmod +x heywhale_submit\n\n# 运行提交工具\n# 把下方 XXXXXXX 替换为你的 Token，submit_file 为要提交的文件名路径\n# 文件名路径去左侧文件树下，刷新，找到对应的 csv 文件，右键复制路径\n!./heywhale_submit -token XXXXXXX -file /home/mw/project/answer4.csv\n","execution_count":null},{"cell_type":"markdown","source":"\n😃\n运行成功、显示提交完成后，即可去[提交页面](https://www.heywhale.com/home/activity/detail/62a07f19ac8fed662502782f/submit)看成绩。满分即可进入下一关。","metadata":{"id":"E33B1692B4924AE9A3E38DD46864445D","notebookId":"62b137eed5964bb28b3198c9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":4}